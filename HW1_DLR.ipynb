{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 Chess:**_  \n",
    "  \n",
    "**environment** - 8x8 checker board  \n",
    "**agent** - the individual moving the pawns   \n",
    "**states** - current board layout    \n",
    "**transitions/actions** - moving pawns (in different ways for each kind of pawn), kicking the others pawns off, gaining a pawn via crossing to the other side with a pawn  \n",
    "**reward** - checkmate on the other players king   \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**1.2 LunarLander:**_  \n",
    "  \n",
    "**environment** - two-dimensional space with a spaceship   \n",
    "**agent** - individual controlling the ship  \n",
    "**state** - point at which the rocket currently is      \n",
    "**transitions/actions** - do nothing, fire left engine, fire main engine, fire right engine  \n",
    "**reward** - successful landing (from the LunarLander page: Reward for moving from the top of the screen to the landing pad and coming to rest is about 100-140 points. If the lander moves away from the landing pad, it loses reward. If the lander crashes, it receives an additional -100 points. If it comes to rest, it receives an additional +100 points. Each leg with ground contact is +10 points. Firing the main engine is -0.3 points each frame. Firing the side engine is -0.03 points each frame. Solved is 200 points.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**1.3 Model-based RL**_  \n",
    "  \n",
    "- **What are environment dynamics?:**  \n",
    "    The environment dynamics of a problem are the reward function and the transition function. For the reward function we need to figure out what the goal state is (and in model-based RL there usually is one) or otherwise what is going to increase our reward. In classical Gridworld, for example, that would be the tile granting positive points and acting as end. Let's say the agent is playing as adventurer and gains the reward once the holy grail is reached. The transition function would then constitute of the moves and actions needed to reach the end tile. For the adventurer it might be to escape traps or defeat enemies before they are able to reach the holy grail at the end. Another example would be the game of tictactoe. There you have the reward of winning the game against your opponent. To achieve that you have take the action of putting your assigned symbol into the free squares, aiming to have three of them within a row, column or in the diagonal next to each other. \n",
    "    ** **\n",
    "- **Can we use that?**\n",
    "    Unfortunately, the environment dynamics are oftentimes not accessible, especially not to determine the optimal sequence of actions to gain the reward. In the adventurer example we cannot determine *the* optimal move, as the adventurer may backtrack or run into deadends from time to time. More importantly the enemies' actions may be random and unpredictable, which would make it impossible to know the optimal sequence of actions. In the TicTacToe example, we have a similar problem. Since we do not know the moves of thge opponent beforehanfs, we cannot ascribe the optimal policy. This is what makes it hard to use the environment dynamics within Reinforcement Learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**2 & 3**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \n",
    "    \"\"\"Gridworld as MDP\"\"\"\n",
    "    def __init__(self, m, n):\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.grid = np.zeros(shape = (m,n))\n",
    "        self.end = False\n",
    "\n",
    "        # possible actions\n",
    "        self.actions = [\"up\", \"down\", \"right\", \"left\"]\n",
    "\n",
    "        #startpoint of the agent\n",
    "        self.grid[1][1] = 5\n",
    "        \n",
    "        #goal position\n",
    "        self.grid[4][3] = 7\n",
    "        \n",
    "        # icy tiles\n",
    "        self.grid[2][2] = 1\n",
    "        self.grid[3][3] = 1\n",
    "        self.grid[4][4] = 1\n",
    "        self.grid[0][1] = 1\n",
    "        self.grid[3][5] = 1\n",
    "\n",
    "        self.reward = 0\n",
    "\n",
    "        # wall tiles\n",
    "        for i in range(self.m):\n",
    "            self.grid[i][0] = 9\n",
    "            self.grid[i][self.n-1] = 9\n",
    "        for j in range(self.n):\n",
    "            self.grid[0][j] = 9\n",
    "            self.grid[self.m-1][j] = 9\n",
    "        \n",
    "        # other wall tiles\n",
    "        self.grid[1][2] = 9\n",
    "        self.grid[1][3] = 9\n",
    "        self.grid[5][4] = 9\n",
    "    \n",
    "    def __getitem__(self, grid):\n",
    "        print(self.grid)\n",
    "        return self.grid\n",
    "\n",
    "    # a function that returns the four tiles that are visible to the agent\n",
    "    def visible_tiles(self):\n",
    "        x,y = self.current_pos()\n",
    "        visible_tiles = []\n",
    "        \n",
    "        for i in self.actions:\n",
    "            next, next_two, con, con2 = self.action_parameter(i)\n",
    "            if con == True:\n",
    "                visible_tiles.append(self.grid[next])\n",
    "        return visible_tiles\n",
    "\n",
    "                \n",
    "     # a function that creates probabilities for the agent to follow and that has no zeros in it that uses only the visible tiles\n",
    "    def probabilities(self):\n",
    "        visible_tiles = self.visible_tiles()\n",
    "        probabilities = []\n",
    "        for i in range(len(visible_tiles)):\n",
    "            # if the agent is seeing an icy tile, the probability of moving in the direction of the edge is 0.25\n",
    "            if visible_tiles[i] == 1:\n",
    "                probabilities.append(0.25)\n",
    "                self.reward -= 0.1\n",
    "            elif visible_tiles[i] == 7:\n",
    "                probabilities.append(1)\n",
    "                self.reward += 10\n",
    "            elif visible_tiles[i] == 9:\n",
    "                probabilities.append(0.1)\n",
    "                self.reward -= 0.1\n",
    "            else:\n",
    "                probabilities.append(0.5)\n",
    "                self.reward -= 0.1\n",
    "        return probabilities, self.reward\n",
    "    \n",
    "    \n",
    "    #state (tile on which the agent is positioned)\n",
    "    def current_pos(self):\n",
    "        found = False\n",
    "        for x in range(self.m):\n",
    "            for y in range(self.n):\n",
    "                if self.grid[x,y] == 5:\n",
    "                    # print the x and y coordinates of the agent\n",
    "                    found = True\n",
    "                    break\n",
    "            if found == True:\n",
    "                break\n",
    "        return x,y\n",
    "\n",
    "    # return random action and activates it\n",
    "    def random_action(self):\n",
    "        action = np.random.choice(self.actions)\n",
    "        self.move(action)\n",
    "        return action\n",
    "       \n",
    "    \n",
    "    def action_parameter(self, action):\n",
    "        x, y = self.current_pos()\n",
    "        con = False\n",
    "        con2 = False\n",
    "        if action == \"up\":\n",
    "            next = x-1,y\n",
    "            next_two = x-2,y\n",
    "            if x-1 >= 0:\n",
    "                con = True\n",
    "            if x-2 >= 0:\n",
    "                con2 = True\n",
    "\n",
    "        elif action == \"down\":\n",
    "            next = x+1,y\n",
    "            next_two = x+2,y\n",
    "            if x+1 >= 0:\n",
    "                con = True\n",
    "            if x+2 <= self.m-1:\n",
    "                con2 = True\n",
    "\n",
    "        elif action == \"right\":\n",
    "            next = x,y+1\n",
    "            next_two = x,y+2\n",
    "            if y+1 <= self.n-1:\n",
    "                con = True\n",
    "            if y+2 <= self.n-1:\n",
    "                con2 = True\n",
    "        elif action == \"left\":\n",
    "            next = x,y-1\n",
    "            next_two = x,y-2\n",
    "            if y-1 >= 0:\n",
    "                con = True\n",
    "            if y-2 >= 0:\n",
    "                con2 = True\n",
    "        return next, next_two, con, con2\n",
    "            \n",
    "    def move(self, action):\n",
    "        next, next_two, con, con2 = self.action_parameter(action)\n",
    "        x, y = self.current_pos()\n",
    "        self.grid[x,y] = 0\n",
    "        # if the agent reaches the goal, the game ends\n",
    "        if self.grid[next] == 7:\n",
    "            self.grid[next] = 5\n",
    "            self.end = True\n",
    "        # if the agent reaches an icy tile, it moves two steps left\n",
    "        elif self.grid[next] == 1 and con2 == True:\n",
    "            if self.grid[next_two] == 7:\n",
    "                self.grid[next_two] == 5\n",
    "                self.end = True\n",
    "            elif self.grid[next_two] == 9:\n",
    "                self.grid[next] = 5\n",
    "            else:\n",
    "                self.grid[next_two] = 5            \n",
    "        # if the agent reaches a wall, it cannot move\n",
    "        elif self.grid[next] == 9:\n",
    "            self.grid[x,y] = 5\n",
    "            #print(\"Wall! Try again.\")\n",
    "        elif self.grid[next] == 0:\n",
    "            self.grid[next] = 5\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def agent():\n",
    "    reward = 0\n",
    "    # probability of the agent to follow the probabilities_no_zeros function\n",
    "    if np.random.uniform(0,1) < 0.5:\n",
    "        probs, reward = world.probabilities()\n",
    "        # print(probs)\n",
    "        # normalize the probabilities\n",
    "        probs = [float(i)/sum(probs) for i in probs]\n",
    "\n",
    "        # choose one of the 4 actions according to the probabilities\n",
    "        action = np.random.choice(world.actions, p=probs)\n",
    "        world.move(action)\n",
    "\n",
    "        return reward\n",
    "        \n",
    "    # probability of the agent to follow the random_action function\n",
    "    else:\n",
    "        world.random_action()\n",
    "        # print(\"random action\")\n",
    "        return reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the policy\n",
    "\n",
    "• Sample at least 1000 episodes of your agent interacting with your self-built\n",
    "GridWorld\n",
    "\n",
    "• For all states s, which have been reached at least once in these episodes,\n",
    "calculate a MC-estimation of Vπ(s) of this state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sample 1000 episodes and calculate a Monte Carlo estimate of the value function for each state\n",
    "world = GridWorld(6,6)\n",
    "def mc(n_episodes):    \n",
    "    # initialize the value function\n",
    "    V = np.zeros((world.m, world.n))\n",
    "    # initialize the number of times each state is visited\n",
    "    N = np.zeros((world.m, world.n))\n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "        # initialize the state\n",
    "        world.__init__(6,6)\n",
    "        # initialize the list of states and rewards\n",
    "        states = []\n",
    "        rewards = []\n",
    "        while world.end == False:\n",
    "            # append the current state and reward to the lists\n",
    "            rewards.append(agent())\n",
    "            states.append(world.current_pos())\n",
    "        # reverse the lists\n",
    "        states.reverse()\n",
    "        rewards.reverse()\n",
    "        \n",
    "        # initialize the return\n",
    "        G = 0\n",
    "        for j in range(len(states)):\n",
    "            # update the return\n",
    "            G = rewards[j] + G\n",
    "            # update the number of times each state is visited\n",
    "            N[states[j]] += 1\n",
    "            # update the value function\n",
    "            V[states[j]] += (G - V[states[j]])/N[states[j]]\n",
    "    return V, N, rewards\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bef8ef1a5544433aabb53a8c67ea034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(array([[  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        , -31.06789734,   0.        ,   0.        ,\n",
       "         -69.24022787,   0.        ],\n",
       "        [  0.        , -28.25534905, -27.29275362, -28.6806962 ,\n",
       "         -46.64663677,   0.        ],\n",
       "        [  0.        , -14.40022936,  -6.64321678,   0.        ,\n",
       "         -17.89804104,   0.        ],\n",
       "        [  0.        ,  13.67972028,  24.48336466,   6.60523466,\n",
       "          25.7908642 ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "           0.        ,  -1.19035874]]),\n",
       " array([[   0.,    0.,    0.,    0.,    0.,    0.],\n",
       "        [   0., 4442.,    0.,    0., 1141.,    0.],\n",
       "        [   0., 4412.,  690., 1896., 1784.,    0.],\n",
       "        [   0., 2616., 1430.,    0., 1072.,    0.],\n",
       "        [   0., 1430., 1064.,  554.,  405.,    0.],\n",
       "        [   0.,    0.,    0.,    0.,    0.,  446.]]),\n",
       " [-4.4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -4.000000000000002,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -3.600000000000002,\n",
       "  0,\n",
       "  -3.2000000000000015,\n",
       "  0,\n",
       "  -2.800000000000001,\n",
       "  0,\n",
       "  0,\n",
       "  -2.400000000000001,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -2.0000000000000004,\n",
       "  -1.6000000000000003,\n",
       "  -1.2,\n",
       "  0,\n",
       "  -0.7999999999999999,\n",
       "  0,\n",
       "  -0.4,\n",
       "  0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc(1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c66be0d3fee8604c3127a165a9c2336ea39dbaaba8a52b5988174e58bb04fdb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
