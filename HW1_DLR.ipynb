{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \n",
    "    \"\"\"Gridworld as MDP\"\"\"\n",
    "    def __init__(self, m, n):\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.grid = np.zeros(shape = (m,n))\n",
    "        self.end = False\n",
    "\n",
    "        # possible actions\n",
    "        self.actions = [\"up\", \"down\", \"right\", \"left\"]\n",
    "\n",
    "        #startpoint of the agent\n",
    "        self.grid[1][1] = 5\n",
    "        \n",
    "        #goal position\n",
    "        self.grid[4][3] = 7\n",
    "        \n",
    "        # icy tiles\n",
    "        self.grid[2][2] = 1\n",
    "        self.grid[3][3] = 1\n",
    "        self.grid[4][4] = 1\n",
    "        self.grid[0][1] = 1\n",
    "        self.grid[3][5] = 1\n",
    "\n",
    "        self.reward = 0\n",
    "\n",
    "        # wall tiles\n",
    "        for i in range(self.m):\n",
    "            self.grid[i][0] = 9\n",
    "            self.grid[i][self.n-1] = 9\n",
    "        for j in range(self.n):\n",
    "            self.grid[0][j] = 9\n",
    "            self.grid[self.m-1][j] = 9\n",
    "        \n",
    "        # other wall tiles\n",
    "        self.grid[1][2] = 9\n",
    "        self.grid[1][3] = 9\n",
    "        self.grid[5][4] = 9\n",
    "    \n",
    "    def __getitem__(self, grid):\n",
    "        print(self.grid)\n",
    "        return self.grid\n",
    "\n",
    "    # a function that returns the four tiles that are visible to the agent\n",
    "    def visible_tiles(self):\n",
    "        x,y = self.current_pos()\n",
    "        visible_tiles = []\n",
    "        \n",
    "        for i in self.actions:\n",
    "            next, next_two, con, con2 = self.action_parameter(i)\n",
    "            if con == True:\n",
    "                visible_tiles.append(self.grid[next])\n",
    "        return visible_tiles\n",
    "\n",
    "                \n",
    "     # a function that creates probabilities for the agent to follow and that has no zeros in it that uses only the visible tiles\n",
    "    def probabilities(self):\n",
    "        visible_tiles = self.visible_tiles()\n",
    "        probabilities = []\n",
    "        for i in range(len(visible_tiles)):\n",
    "            # if the agent is seeing an icy tile, the probability of moving in the direction of the edge is 0.25\n",
    "            if visible_tiles[i] == 1:\n",
    "                probabilities.append(0.25)\n",
    "                self.reward -= 0.1\n",
    "            elif visible_tiles[i] == 7:\n",
    "                probabilities.append(1)\n",
    "                self.reward += 10\n",
    "            elif visible_tiles[i] == 9:\n",
    "                probabilities.append(0.1)\n",
    "                self.reward -= 0.1\n",
    "            else:\n",
    "                probabilities.append(0.5)\n",
    "                self.reward -= 0.1\n",
    "        return probabilities, self.reward\n",
    "    \n",
    "    \n",
    "    #state (tile on which the agent is positioned)\n",
    "    def current_pos(self):\n",
    "        found = False\n",
    "        for x in range(self.m):\n",
    "            for y in range(self.n):\n",
    "                if self.grid[x,y] == 5:\n",
    "                    # print the x and y coordinates of the agent\n",
    "                    found = True\n",
    "                    break\n",
    "            if found == True:\n",
    "                break\n",
    "        return x,y\n",
    "\n",
    "    # return random action and activates it\n",
    "    def random_action(self):\n",
    "        action = np.random.choice(self.actions)\n",
    "        self.move(action)\n",
    "        return action\n",
    "       \n",
    "    \n",
    "    def action_parameter(self, action):\n",
    "        x, y = self.current_pos()\n",
    "        con = False\n",
    "        con2 = False\n",
    "        if action == \"up\":\n",
    "            next = x-1,y\n",
    "            next_two = x-2,y\n",
    "            if x-1 >= 0:\n",
    "                con = True\n",
    "            if x-2 >= 0:\n",
    "                con2 = True\n",
    "\n",
    "        elif action == \"down\":\n",
    "            next = x+1,y\n",
    "            next_two = x+2,y\n",
    "            if x+1 >= 0:\n",
    "                con = True\n",
    "            if x+2 <= self.m-1:\n",
    "                con2 = True\n",
    "\n",
    "        elif action == \"right\":\n",
    "            next = x,y+1\n",
    "            next_two = x,y+2\n",
    "            if y+1 <= self.n-1:\n",
    "                con = True\n",
    "            if y+2 <= self.n-1:\n",
    "                con2 = True\n",
    "        elif action == \"left\":\n",
    "            next = x,y-1\n",
    "            next_two = x,y-2\n",
    "            if y-1 >= 0:\n",
    "                con = True\n",
    "            if y-2 >= 0:\n",
    "                con2 = True\n",
    "        return next, next_two, con, con2\n",
    "            \n",
    "    def move(self, action):\n",
    "        next, next_two, con, con2 = self.action_parameter(action)\n",
    "        x, y = self.current_pos()\n",
    "        self.grid[x,y] = 0\n",
    "        # if the agent reaches the goal, the game ends\n",
    "        if self.grid[next] == 7:\n",
    "            self.grid[next] = 5\n",
    "            self.end = True\n",
    "        # if the agent reaches an icy tile, it moves two steps left\n",
    "        elif self.grid[next] == 1 and con2 == True:\n",
    "            if self.grid[next_two] == 7:\n",
    "                self.grid[next_two] == 5\n",
    "                self.end = True\n",
    "            elif self.grid[next_two] == 9:\n",
    "                self.grid[next] = 5\n",
    "            else:\n",
    "                self.grid[next_two] = 5            \n",
    "        # if the agent reaches a wall, it cannot move\n",
    "        elif self.grid[next] == 9:\n",
    "            self.grid[x,y] = 5\n",
    "            #print(\"Wall! Try again.\")\n",
    "        elif self.grid[next] == 0:\n",
    "            self.grid[next] = 5\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def agent():\n",
    "    reward = 0\n",
    "    # probability of the agent to follow the probabilities_no_zeros function\n",
    "    if np.random.uniform(0,1) < 0.5:\n",
    "        probs, reward = world.probabilities()\n",
    "        # print(probs)\n",
    "        # normalize the probabilities\n",
    "        probs = [float(i)/sum(probs) for i in probs]\n",
    "\n",
    "        # choose one of the 4 actions according to the probabilities\n",
    "        action = np.random.choice(world.actions, p=probs)\n",
    "        world.move(action)\n",
    "\n",
    "        return reward\n",
    "        \n",
    "    # probability of the agent to follow the random_action function\n",
    "    else:\n",
    "        world.random_action()\n",
    "        # print(\"random action\")\n",
    "        return reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the policy\n",
    "\n",
    "• Sample at least 1000 episodes of your agent interacting with your self-built\n",
    "GridWorld\n",
    "\n",
    "• For all states s, which have been reached at least once in these episodes,\n",
    "calculate a MC-estimation of Vπ(s) of this state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sample 1000 episodes and calculate a Monte Carlo estimate of the value function for each state\n",
    "world = GridWorld(6,6)\n",
    "def mc(n_episodes):    \n",
    "    # initialize the value function\n",
    "    V = np.zeros((world.m, world.n))\n",
    "    # initialize the number of times each state is visited\n",
    "    N = np.zeros((world.m, world.n))\n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "        # initialize the state\n",
    "        world.__init__(6,6)\n",
    "        # initialize the list of states and rewards\n",
    "        states = []\n",
    "        rewards = []\n",
    "        while world.end == False:\n",
    "            # append the current state and reward to the lists\n",
    "            rewards.append(agent())\n",
    "            states.append(world.current_pos())\n",
    "        # reverse the lists\n",
    "        states.reverse()\n",
    "        rewards.reverse()\n",
    "        \n",
    "        # initialize the return\n",
    "        G = 0\n",
    "        for j in range(len(states)):\n",
    "            # update the return\n",
    "            G = rewards[j] + G\n",
    "            # update the number of times each state is visited\n",
    "            N[states[j]] += 1\n",
    "            # update the value function\n",
    "            V[states[j]] += (G - V[states[j]])/N[states[j]]\n",
    "    return V, N, rewards\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bef8ef1a5544433aabb53a8c67ea034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(array([[  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        , -31.06789734,   0.        ,   0.        ,\n",
       "         -69.24022787,   0.        ],\n",
       "        [  0.        , -28.25534905, -27.29275362, -28.6806962 ,\n",
       "         -46.64663677,   0.        ],\n",
       "        [  0.        , -14.40022936,  -6.64321678,   0.        ,\n",
       "         -17.89804104,   0.        ],\n",
       "        [  0.        ,  13.67972028,  24.48336466,   6.60523466,\n",
       "          25.7908642 ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "           0.        ,  -1.19035874]]),\n",
       " array([[   0.,    0.,    0.,    0.,    0.,    0.],\n",
       "        [   0., 4442.,    0.,    0., 1141.,    0.],\n",
       "        [   0., 4412.,  690., 1896., 1784.,    0.],\n",
       "        [   0., 2616., 1430.,    0., 1072.,    0.],\n",
       "        [   0., 1430., 1064.,  554.,  405.,    0.],\n",
       "        [   0.,    0.,    0.,    0.,    0.,  446.]]),\n",
       " [-4.4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -4.000000000000002,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -3.600000000000002,\n",
       "  0,\n",
       "  -3.2000000000000015,\n",
       "  0,\n",
       "  -2.800000000000001,\n",
       "  0,\n",
       "  0,\n",
       "  -2.400000000000001,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -2.0000000000000004,\n",
       "  -1.6000000000000003,\n",
       "  -1.2,\n",
       "  0,\n",
       "  -0.7999999999999999,\n",
       "  0,\n",
       "  -0.4,\n",
       "  0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc(1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c66be0d3fee8604c3127a165a9c2336ea39dbaaba8a52b5988174e58bb04fdb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
