{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \n",
    "    \"\"\"Gridworld as MDP\"\"\"\n",
    "    def __init__(self, m, n):\n",
    "        \"\"\" initializes the gridworld\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.grid = np.zeros(shape = (m,n))\n",
    "        self.end = False\n",
    "        self.goal_tile = 60\n",
    "        self.wall_tile = 100\n",
    "        self.ice_tile = 30\n",
    "        self.figure = 0\n",
    "        self.free_tile = 230\n",
    "        # fill the grid with free tiles\n",
    "        for i in range(self.m):\n",
    "            for j in range(self.n):\n",
    "                self.grid[i,j] = self.free_tile\n",
    "        # possible actions\n",
    "        self.actions = [\"up\", \"down\", \"right\", \"left\"]\n",
    "\n",
    "        #startpoint of the agent\n",
    "        self.grid[0][0] = self.figure\n",
    "        \n",
    "        #goal position\n",
    "        self.grid[self.m-1][self.n-1] = self.goal_tile\n",
    "    \n",
    "        self.grid[1][1] = self.ice_tile\n",
    "        self.grid[1][4]= self.ice_tile\n",
    "        self.grid[1][3] = self.wall_tile\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\" resets the gridworld\n",
    "        \"\"\"\n",
    "        self.end = False\n",
    "        #startpoint of the agent\n",
    "        self.grid[0][0] = self.figure\n",
    "        \n",
    "        #goal position\n",
    "        self.grid[self.m-1][self.n-1] = self.goal_tile\n",
    "        \n",
    "        self.grid[1][1] = self.ice_tile\n",
    "        self.grid[1][4] = self.ice_tile\n",
    "\n",
    "    def __getitem__(self, grid):\n",
    "        \"\"\" returns the grid\n",
    "\n",
    "        Args:\n",
    "            grid (np_array): gridworld\n",
    "        \"\"\"\n",
    "        print(self.grid)\n",
    "        return self.grid\n",
    "\n",
    "    # a function that returns the four tiles that are visible to the agent\n",
    "    def visible_tiles(self):\n",
    "        \"\"\" returns the four tiles that are visible to the agent\n",
    "        \"\"\"\n",
    "        x,y = self.current_pos()\n",
    "        visible_tiles = []\n",
    "        for i in self.actions:\n",
    "            next, next_two, con, con2 = self.action_parameter(i)\n",
    "            if con == True:\n",
    "                visible_tiles.append(self.grid[next])\n",
    "        return visible_tiles\n",
    "        \n",
    "    def visualize_grid(self):\n",
    "        \"\"\" visualizes the gridworld\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.matshow(self.grid, cmap = 'Pastel1')\n",
    "        for i in range(self.grid.shape[1]):\n",
    "            for j in range(self.grid.shape[0]):\n",
    "                # create a string for the label of each tile in the right format\n",
    "                if str(self.grid[j,i]) == str(float(self.free_tile)):\n",
    "                    label = \" \"\n",
    "                elif str(self.grid[j,i]) == str(float(self.wall_tile)):\n",
    "                    label = \"wall\"\n",
    "                elif str(self.grid[j,i]) == str(float(self.ice_tile)):\n",
    "                    label = \"ice\"\n",
    "                elif str(self.grid[j,i]) == str(float(self.goal_tile)):\n",
    "                    label = \"goal\"\n",
    "                elif str(self.grid[j,i]) == str(float(self.figure)):\n",
    "                    label = \"agent\"\n",
    "                else:\n",
    "                    label = str(self.grid[j,i])\n",
    "                ax.text(i, j, label, va = 'center', ha = 'center')\n",
    "        plt.show()\n",
    "\n",
    "    def current_pos(self):\n",
    "        \"\"\" returns the current position of the agent\n",
    "        \"\"\"\n",
    "        found = False\n",
    "        for x in range(self.m):\n",
    "            for y in range(self.n):\n",
    "                if self.grid[x,y] == self.figure:\n",
    "                    found = True\n",
    "                    break\n",
    "            if found == True:\n",
    "                break\n",
    "        return x,y\n",
    "    \n",
    "    def action_parameter(self, action):\n",
    "        \"\"\" returns the next tile and the tile after the next tile and if the next tile is in the grid\n",
    "\n",
    "        Args:\n",
    "            action (str): action that the agent should take\n",
    "        \"\"\"\n",
    "        x, y = self.current_pos()\n",
    "        con = False\n",
    "        con2 = False\n",
    "        if action == \"up\":\n",
    "            next = x-1,y\n",
    "            next_two = x-2,y\n",
    "            if x-1 >= 0:\n",
    "                con = True\n",
    "            if x-2 >= 0:\n",
    "                con2 = True\n",
    "\n",
    "        elif action == \"down\":\n",
    "            next = x+1,y\n",
    "            next_two = x+2,y\n",
    "            if x+1 <= self.m-1:\n",
    "                con = True\n",
    "            if x+2 <= self.m-1:\n",
    "                con2 = True\n",
    "\n",
    "        elif action == \"right\":\n",
    "            next = x,y+1\n",
    "            next_two = x,y+2\n",
    "            if y+1 <= self.n-1:\n",
    "                con = True\n",
    "            if y+2 <= self.n-1:\n",
    "                con2 = True\n",
    "        elif action == \"left\":\n",
    "            next = x,y-1\n",
    "            next_two = x,y-2\n",
    "            if y-1 >= 0:\n",
    "                con = True\n",
    "            if y-2 >= 0:\n",
    "                con2 = True\n",
    "        return next, next_two, con, con2\n",
    "            \n",
    "    def move(self, action):\n",
    "        \"\"\" move the agent according to the action\n",
    "\n",
    "        Args:\n",
    "            action (str):  action that the agent should take\n",
    "        \"\"\"\n",
    "        if action != None:\n",
    "            next, next_two, con, con2 = self.action_parameter(action)\n",
    "            x, y = self.current_pos()\n",
    "            if con == True:\n",
    "                self.grid[x,y] = self.free_tile\n",
    "                # if the agent reaches the goal, the game ends\n",
    "                if self.grid[next] == self.goal_tile:\n",
    "                    # self.grid[next] = self.figure\n",
    "                    self.end = True\n",
    "                # if the agent reaches an icy tile, it moves two steps left\n",
    "                elif self.grid[next] == self.ice_tile and con2 == True:\n",
    "                    if self.grid[next_two] == self.goal_tile:\n",
    "                        # self.grid[next_two] == self.figure\n",
    "                        self.end = True\n",
    "                    elif self.grid[next_two] == self.wall_tile:\n",
    "                        self.grid[next] = self.figure\n",
    "                    else:\n",
    "                        self.grid[next_two] = self.figure          \n",
    "                # if the agent reaches a wall, it cannot move\n",
    "                elif self.grid[next] == self.wall_tile:\n",
    "                    self.grid[x,y] = self.figure\n",
    "                elif self.grid[next] == self.free_tile:\n",
    "                    self.grid[next] = self.figure\n",
    "\n",
    "    def get_reward(self, next):\n",
    "        \"\"\" Returns the reward of the agent\n",
    "\n",
    "        Args:\n",
    "            next (tuple): next position of the agent\n",
    "        \"\"\"\n",
    "        if next == self.goal_tile:\n",
    "            return 100\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def initialize_policy(self):\n",
    "        \"\"\" Initializes the policy of the agent\n",
    "        \"\"\"\n",
    "        # initialize the policy with random values\n",
    "        policy = [[0 for column in range(self.n)] for row in range(self.m)]\n",
    "        #iterate over all states\n",
    "        for row in range(self.n):\n",
    "            for column in range(self.m):\n",
    "                #equally distribute probability among possible actions\n",
    "                policy[row][column] = [(1/self.actions) for a in self.actions]\n",
    "        return policy\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def policy_evaluation(self, grid, policy, value):\n",
    "        \"\"\" Policy evaluation function for the policy iteration algorithm\n",
    "\n",
    "        Args:\n",
    "            grid (np_array): grid of the environment\n",
    "            policy (np_array): policy of the agent\n",
    "            value (np_array): value function of the agent\n",
    "        \"\"\"\n",
    "        # with convergence and for the policy iteration in the function mc_estimation\n",
    "        for i in range(self.m):\n",
    "            for j in range(self.n):\n",
    "                grid[i,j] = self.grid[i,j]\n",
    "        # initialize delta\n",
    "        delta = 1\n",
    "        # while delta is greater than the threshold\n",
    "        while delta > 0.01:\n",
    "            delta = 0\n",
    "            # for all states\n",
    "            for i in range(self.m):\n",
    "                for j in range(self.n):\n",
    "                    # keep the old value\n",
    "                    old_value = value[i,j]\n",
    "                    # initialize the new value\n",
    "                    new_value = 0\n",
    "                    # for all actions\n",
    "                    for k in range(len(self.actions)):\n",
    "                        # get the next tile and the tile after the next tile\n",
    "                        next, next_two, con, con2 = self.action_parameter(self.actions[k])\n",
    "                        # if the next tile is in the grid\n",
    "                        if con == True:\n",
    "                            # update the new value\n",
    "                            new_value += policy[i,j,k] * (self.get_reward(next) + 0.9 * value[next])\n",
    "                    # update the value function\n",
    "                    value[i,j] = new_value\n",
    "                    # update delta\n",
    "                    delta = max(delta, abs(old_value - value[i,j]))\n",
    "        return value\n",
    "\n",
    "        #new_value += policy[i,j,k] * (self.get_reward(next_two) + 0.9 * value[next_two])\n",
    "\n",
    "    def policy_improvement(self, grid, policy, value, data):\n",
    "        \"\"\"Policy improvement step of policy iteration algorithm\n",
    "\n",
    "        Args:\n",
    "            grid (np_array): grid of the environment\n",
    "            policy (np_array): policy of the agent\n",
    "            value (np_array): value function of the agent\n",
    "        # \"\"\"\n",
    "        policy_stable = True\n",
    "        # for all states\n",
    "        for i in range(self.m):\n",
    "            for j in range(self.n):\n",
    "                # keep the old action\n",
    "                old_action = np.argmax(policy[i,j])\n",
    "                # initialize the new action\n",
    "                new_action = 0\n",
    "                # initialize the new value\n",
    "                new_value = 0\n",
    "                # for all actions\n",
    "                for k in range(len(self.actions)):\n",
    "                    # get the next tile and the tile after the next tile\n",
    "                    next, next_two, con, con2 = self.action_parameter(self.actions[k])\n",
    "                    # if the next tile is in the grid\n",
    "                    if con == True:\n",
    "                        # update the new value\n",
    "                        new_value += policy[i,j,k] * (self.get_reward(next) + 0.9 * value[next])\n",
    "                # update the action\n",
    "                new_action = np.argmax(new_value)\n",
    "                # update the policy\n",
    "                policy[i,j,old_action] = 0\n",
    "                policy[i,j,new_action] = 1\n",
    "                # if the policy is not stable\n",
    "                if old_action.all() != new_action:\n",
    "                    policy_stable = False\n",
    "        return policy, policy_stable\n",
    "\n",
    "\n",
    "    def mc_estimation(self, num_episodes, gamma, epsilon):\n",
    "        \"\"\"Monte Carlo estimation\n",
    "\n",
    "        Args:\n",
    "            num_episodes (int): number of episodes\n",
    "            gamma (int): discount factor\n",
    "            epsilon (int): epsilon\n",
    "        \"\"\"\n",
    "        # initialize the value function\n",
    "        value = np.zeros(shape = (self.m, self.n))\n",
    "        # initialize the policy\n",
    "        policy = self.initialize_policy()\n",
    "        #Q = np.zeros(shape = (self.m, self.n, len(self.actions)))\n",
    "        average_returns = []\n",
    "        # time\n",
    "        time_per_episode = []\n",
    "        returns = {}\n",
    "        \n",
    "        for i in tqdm(range(num_episodes)):\n",
    "            # initialize the episode\n",
    "            G = 0\n",
    "            episode = []\n",
    "            seen_states = []\n",
    "            self.end = False\n",
    "            policy_stable = False\n",
    "            rewards = []\n",
    "            # returns = {}\n",
    "            self.reset()\n",
    "            # time each episode\n",
    "            time_start = time.time()\n",
    "            # loop over the episode\n",
    "            while self.end == False:\n",
    "                # choose an action\n",
    "                old_position = self.current_pos\n",
    "                action = np.random.choice(self.actions, p = policy[self.current_pos()])\n",
    "                # p = 0\n",
    "                # p_a = []\n",
    "                # for s in range(len(self.actions)):\n",
    "                #     if policy[self.current_pos()][s] > 0:\n",
    "                #         p += policy[self.current_pos()][s]\n",
    "                #         p_a.append(s)\n",
    "                        \n",
    "                      \n",
    "                # self.visualize_grid()\n",
    "                # choose random int from p_a\n",
    "                action = self.actions[np.random.choice(p_a)]\n",
    "\n",
    "                # take the action and observe the next state and reward\n",
    "                self.move(action)\n",
    "                # add the reward to the return\n",
    "                G += self.get_reward(self.grid[self.current_pos()]) + gamma * G\n",
    "                # add the state to the episode\n",
    "                episode.append((self.current_pos()))\n",
    "                # check if the episode is done\n",
    "                if self.current_pos() == self.goal_tile:\n",
    "                    self.end = True\n",
    "                # add the reward to the rewards list\n",
    "                rewards.append(self.get_reward(self.grid[self.current_pos()]))\n",
    "            # end time\n",
    "            time_end = time.time()\n",
    "            # calculate the time\n",
    "            time_episode = time_end - time_start\n",
    "            # add the time to the time list\n",
    "            time_per_episode.append(time_episode)\n",
    "            #if self.end == True:\n",
    "                # loop over the episode\n",
    "            for i in range(len(episode)):\n",
    "                # if the state is not in the returns list\n",
    "                if episode[i] not in returns:\n",
    "                    # add the state to the returns list\n",
    "                    returns[episode[i]] = []                    \n",
    "                # if the state is in the returns list\n",
    "                else:\n",
    "                    # add the return to the returns list\n",
    "                    returns[episode[i]].append(G)\n",
    "                    # calculate the value function\n",
    "                    value[episode[i]] = np.mean(returns[episode[i]])\n",
    "                    # calculate the average return\n",
    "                # break the loop\n",
    "            \n",
    "            average_returns.append(np.mean(returns[episode[i]]))\n",
    "\n",
    "            while policy_stable == False:      \n",
    "                # update the policy\n",
    "                # evaluate the policy\n",
    "                value = self.policy_evaluation(self.grid, policy, value)\n",
    "                policy, policy_stable = self.policy_improvement(self.grid, policy, value, returns)\n",
    "                # normalize the policy between 0 and 1\n",
    "                \n",
    "                # for i in range(self.m):\n",
    "                #     for j in range(self.n):\n",
    "                #         for k in range(len(self.actions)):\n",
    "                #             if policy[i,j,k] != 0:\n",
    "                #                 policy[i,j,k] = policy[i,j, k] / np.argmax(policy)\n",
    "                        \n",
    "        return policy, returns, value, average_returns, time_per_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld(4,5)\n",
    "env.visualize_grid()\n",
    "policy = env.initialize_policy()\n",
    "policy, returns, value, average_returns, time_episode = env.mc_estimation(1000, 0.94, 0.1)\n",
    "print(policy)\n",
    "print(value)\n",
    "# self.actions = [\"up\", \"down\", \"right\", \"left\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize average return-per-episode in episodes-sampled\n",
    "import matplotlib.pyplot as plt\n",
    "#visualize average return-per-episode in episodes-sampled\n",
    "x = list(range(1000))\n",
    "y = average_returns\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Episodes sampled')\n",
    "plt.ylabel('Average return-per-episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the time-per-episode in episodes-sampled\n",
    "x = list(range(1000))\n",
    "y = time_episode\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Episodes sampled')\n",
    "plt.ylabel('Time-per-episode')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd3405dfa4d08822a6a463cc4cd49794da5171eabfa950028dcc45f530d8f018"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
